\documentclass[midd]{thesis}

\usepackage{graphicx}
\usepackage{times}

\bibliographystyle{plain}

\title {Geolocation through Language Recognition}

\author {Will Potter}
\adviser {Professor David Kauchak}

\begin{document}

\maketitle

\begin{abstract}
With an increasing amount of text being shared on the web, through blogs, social media, websites pictures, it is becoming increasingly more difficult to translate the text in these mediums into geographic coordinates and physical locations. While GPS-enabled devices are becoming more popular, many people with mobile phones would prefer to not share their locations with applications and companies. Additionally, IP geolocation lacks the precision that GPS-enabled devices have. Yet, while Internet users don't explicitly share their GPS-location, they often will share information about their location in the form of textual status updates. Using geotagged tweets and other geotagged information, it should be possible to identify similarities between non-geotagged texts and classify someone's location by the words included in their tweet. With this information, more intelligence can be gathered about people tweeting, even if they haven't included their specific geographic coordinates with the tweet.

This thesis will focus specifically on classifying text to a variety of regions, including countries, states, counties and towns. It will use a variety of supervised learning classification techniques including SVM's and Naive Bayes. While the classifier is importance, the study will also focus on feature preprocessing, as that will most likely have a great impact on the results of the final product.
\end{abstract}

\begin{acknowledgements}
% 
\end{acknowledgements}

\contentspage
\tablelistpage
\figurelistpage

\normalspacing \setcounter{page}{1} \pagenumbering{arabic}

\chapter{Introduction}

\section{Classification of Social Media}

The rapid growth of social media in the past 10 years has shown that people are living their lives increasingly more through the internet and are sharing more information with more people than ever before in history. At the end of 2013, Facebook had 1.23 billion monthly active users \cite{facebookabout} and Twitter had 241 million monthly active users \cite{twitterabout} -- and those are just the largest two networks. People are sharing content online through Pinterest, Foursquare, Tumblr and many other sites. Yet while the number of social media users has expanded, much about their behavior is still difficult to determine.

As people share information, companies analyze user information to help data monetization efforts, such as advertising. For example, if 95\% of a company’s users were under the age of 30, advertising for retirement communities wouldn't be an efficient use of advertising space. They also review user behavior to improve their product and determine where efforts should be expanded. If only 3\% of Facebook users were to use Facebook chat, spending effort elsewhere would be prudent. 

While this principle seems similar, analyzing this data becomes increasingly more complex as the size of the data increases -- Twitter, alone, manages 500 million tweets per day. Deriving useful conclusions from such a large dataset requires an intricate knowledge of the data before effective analysis can be applied.

\section{Smartphones and Geotagging}
In addition to the growth of social media, the world has seen the adoption of the Internet connected smartphone. Business Insider found that 22\% of the global population owned a smartphone at the end of 2013. Quite simply, people are sharing more and doing so wherever they go. As someone uses their phone, they also have the ability to share their phone's GPS location and attach it to their social media updates. This premise gave way to the creation of Foursquare, a geographic social network that allows users to share their location with other users and local businesses for perks. As Foursquare refined their idea, Facebook and Twitter soon followed with location-sharing services inside their mobile apps. Even though these features are now a part of most social media apps, adoption has not be excellent. Estimates show that only 1 in 5 tweets are geotagged, which still beats Facebook’s poor geographic integration (Need Better Citation).

Geotagging is particularly useful for advertising, as well as business intelligence. Twitter, while an incomplete sample of the population, can serve as a tracking agent for businesses. While the requirement that people ``opt-in" to sharing their location may be a bias in the data, companies still may use geotagged tweets as a random sample of all social media users when running analytics to generalize conclusions across an entire user base. For example, McDonald's might like to show an advertisement for someone who works immediately next to one, but has never tweeted or checked-in at a their restaurant. A user's proximity to a certain business makes them a more appealing ad target, which in turn provides a premium experience for advertisers to use. A McDonald's or Target would greatly prefer to target people who live nearby or pass through town rather than simply targeting based on a user's interests, tweets or follows.

Given that Twitter only records a definitive location for 20\% of their tweets, they lose out on location information for the rest of the almost 80\% of their users. If a social network were to mandate location sharing, the company would be perceived as grossly violating individual privacy rights and would almost certainly lose many users, thus decreasing the underlying value of their service. Even if the locations were automatically collected and kept private, smartphone manufacturers, like Apple, could disable GPS-use on their phones to protect users. If a social network had some way at guessing the location of a user based purely based on the content of their update (i.e. a tweet's body or a Facebook status' text), they would be able to provide a superior analytic experience without storing the exact geographic coordinates of a user. Additionally, the network would be using information that the user implicitly consents to public sharing (by nature of using a social network in the first place).

This thesis looks to examine the possibility of determining a user's location at the point of sharing (or close to the point of sharing) based on text classification and clustering around the content of their update. Specifically, it will use a set of geotagged tweets acquired from Twitter's public API as training data for running a series of machine learning algorithms on the text of tweets. By training on already geo-tagged tweets, which appear to have virtually the same content as non-geotagged tweets, anyone could later predict locations for the remaining 80\% of tweets. This thesis will focus on English language tweets, as tagged by the Twitter API, but the methods could hopefully carry over to other languages as well.

While initial efforts target exclusively the body text of tweets, incorporating user information into classification should improve results. Accounting for the location of other tweets by the same user and the user's stated location in their settings should help influence the classification result especially if classification efforts narrowed it down to a small number of places. Additionally, using the time as a feature should help, as most people operate in regular cycles (daytime at a place of work or school, and nighttime at home).

As many words bear certain significance to a particular geographic location, it can be expected that different types of words will have an effect on predicting the location of a tweet. At the simplest level, referring to place names, such as New York, Boston, or San Francisco, can be expected to have a correlation to the location of the tweeter. It may relate to something in the past (``Just got back from New York City...great weekend."), present (``I'm at New York Hilton Midtown - @hiltonhotels (New York, NY)"), future (``3 hours to go until New York will be calling! \#fashion \#opportunity \#career"). The user may not even be planning to go to the place, but rather just is referencing the place (``If only New York wasn't so far away").

Additionally, the use of neighboorhoods or other place names may indicate a geographic location. Tweets like ``I ❤️ SNOW \#nyc @ Hell's Kitchen - NYC" refer to the Hell's Kitchen neighboorhood in NYC but tweets refer to other cultural features, like the TV show ``Hell's Kitchen": ``Literally can't get enough of @GordonRamsay `a Hell's Kitchen' - absolute stormer of a show, can't wait for the new series".

Finally, the presence of particular words, like ``frappe" and ``milkshake" may indicate if someone is in a particular location. Tweets with ``frappe" are less popular and appear in the Northeast mainly, while ``milkshake" appears more frequently and across a greater geographic area.

Using a fair degree of knowledge about the use cases of Twitter, we can hope to see some success in analyzing a tweet's textual body to infer the location of a user.

\chapter{Related Work}

Founded in 2007, Twitter is a relatively new platform. Like most parts of the high tech sector, it has changed and evolved rapidly, starting as a text messaging based app that ultimately transitioned to mobile and web application based network. As Twitter has grown, it has seen increased interest from academics for its representation of realtime human dialogue and movement. Kwak et al. \cite{kwak2010twitter} found that Twitter had transformed into a hybrid social network and news medium. Given the low user reciprocity around sharing as well as the high average retweet count for an average tweet, it resembles a crowd-sourced news network almost as much as a person-to-person network.

Building on the crowd sourced news network description, researchers created systems that record and plot geo-tagged tweets and photos on a map, to show a heatmap effect regarding Twitter activity at any given moment. \cite{nakaji2012visualization} \cite{yanai2012world}

Kinsella et al. \cite{kinsella2011m} started to consider the idea of geotagging tweets by comparing it with several other forms of geotagging. Using a baseline constructed from a tweet's GPS coordinates, they considered geotagging based on a user's self reported location as well as a tweets content. They then compared this to the majority label in a given dataset.

Twitter's ability to analyze realtime communication across the globe has allured researchers to consider various questions, especially where users are when they tweet. Despite Twitter's included GPS coordinates, others have tried to extract location information from a tweets content. Using a highly specific dataset consisiting of Twitter users with over 1000 tweets and a listed profile location in the continental United States, Cheng et al. \cite{cheng2010you} were able to achieve an accuracy rate of 51\% when classifiying user's locations. Initially, they identified local words, such as ``tortilla'', ``redsox'' or ``canyon''. By combining the geographic centers of each of these words, they probabalistically determined a new centroid for a likely area of the tweet in question. 

Roller et al. \cite{roller2012supervised} compared the text from tweets to definitive, fact-heavy text from Wikipedia downloaded. Their experiments could correctly geolocate the tweets with 161km of its true location 90\% of the time. By using a k-dimensional tree, they recursively broke up the geographic grid to best identify which node fit the testing data. Training data included the wikipedia data containing many place names and toponyms. They then compared a tweet to the wikipedia text, knowing that placenames included in tweets would correlate to a particular document from wikipedia often.

Additionally, the tweet corpus has been analyzed to search for spatiotemporal anomalies, such as natural disasters or societal events, like riots or protests through clustering tweets across time. Their study produced relevant visualizations that would accurately overlay the type of disturbance on a map, based on the trending terms used in tweets. \cite{thom2012spatiotemporal} 


\chapter{Data and Preprocessing}
\section{Data}

Data was collected from the Twitter API by querying their streaming API for all tweets with an attached pair of geographic coordinates or an attached geofenced region. Tweets with a region attached were assigned the midpoint of the region. A program, running on a personal computer would run for a period of time downloading new tweets during that period and storing them in a database. 

The dataset is comprised of 1,656,146 tweets with 612,728 unique users tweeting within a period of February 13, 2014 and March 6th, 2014. This sample represents all geotagged, English-language tweets collected over random time intervals during the above period. Collection from the Twitter API occasionally times out, leaving gaps where tweets were not collected. As Twitter publishes approximately 500 million tweets per day, this dataset of 1.6 million represents a vast minority of tweets between the dates of collection, yet it still provides a significant number for running experiments.

\section{Preprocessing}
Tweets tend to be informal and are filled with insignificant, popular words (``the", ``I", ``a") as well as a range of misspellings. In addition, tweets contain links and usernames that don't necessarily have a strong correlation to one's location. For this reason, simply counting each word as a feature is a naïve approach that introduces an unwanted amount of noise to classification experiments. Additionally, examining bigrams, or particular phrases, as opposed to simply looking at 1-gram words is expected to increase the accuracy of classification. The bigram, ``in Boston", is more likely to imply the user is actually in Boston compared to ``from Boston".

\section{Labeling}
Tweets are labeled with geographic coordinates or regions pertaining to particular cities or areas. The geographic locations come from mobile phone GPS units or Twitter's own location detection attempts that incorporate IP addresses as well as some textual features.

\chapter{Experimental Methods}

\section{Posing the question as a classification problem}
Taking the dataset of geotagged tweets, this study structures its experiments as a classification problem. The tweets in question are already geotagged, thus providing a definitive definition of where they were written. With this in mind, the goal is to predict the geographic coordinates of tweets without included geodata.

\section{Scope}
While the dataset only comprises English language tweets, tweets are saved from around the world. Given the lack of English speakers in many regions of the world, it could be problematic to label the entire world, as a region in Africa will have substantially fewer training examples than a region in Fairfield County, CT would have. Experiments will examine the continental US separately in addition to attempting worldwide classification.

\section{Labels}
This study examines several different methodologies for labeling tweets.

\subsection{Geographic Grid Regions}
In running experiments, labels are derived from the geographic coordinates of a tweet. As the labels run across as wide range of real numbers in 2 dimensions, effectively labeling the training features is a non-trivial process. The easiest way to label tweets is to break up the geographic into regions of equal latitude and longitude intervals. This, however, ignores cultural boundaries and dense areas. A 1-degree x 1-degree ``bucket" in Montana has a smaller and more homogenous population with fewer places. However, the ``bucket" including New York City would have many more people, places and material and determining if a tweet originated there would be difficult. Given the inequality of area as latitudes change, this can be expected to behave differently for regions on the equator when compared with polar regions. While most of the world's population and tweets do not originate from the extreme poles, northern/southern regions will be broken into smaller buckets than central areas.

\subsection{Political Boundaries}
Using political boundaries or census designated places, as labels would be a slightly more intelligent way to break up the corpus of tweets, but still doesn't account for a large difference in population density. Breaking up a heavily populated state like Massachusetts or New Jersey into more buckets than Montana or Wyoming would give more authentic labels, due to the diversity and increased activity on social media. Additionally, latitude and longitude are often not the most effective ways of differentiating a population as cultural boundaries are often not straight lines.

\subsection{N-sized Example Buckets}
Using a k-dimensional tree to break up tweets into n-sized buckets by their latitude and longitude, experiments will break the dataset into labels that more accurately represent population distribution across the planet. This method is similar to the work of Roller et al. \cite{roller2012supervised}, except it will train the model on tweets rather than Wikipedia data.

\section{Data Training and Testing Protocols}

\subsection{Randomness and Cross Validation}
Due to performance implications, most experiments ran with 10000 examples. From the 1.6 million-tweet corpus, tweets are randomly selected for each experiment. All models then run on the same random, 10000-example sample to avoid variation that might stem from pulling drastically different sets from the complete corpus.

Additionally, to account for variation that might occur within 1 experiment, 10-fold cross validation is used rather than simply running testing the whole sample in one go. Within each fold, the data is split into 80\% training and 20\% testing.

\section{Supervised Learning Models}
\subsection{K-Nearest Neighbor}

K-Nearest Neighbor classification examines a set of training examples and using their features and internally stores them on an n-dimensional space. In our case, features are words or n-grams included in the body text of tweets. Then with a test example, the model identifies the K-nearest examples and chooses the majority label.

In order to mitigate the risk of insignificant terms weighting the distance metrics for each example, a term frequency-inverse document frequency (TF-IDF) vectorizer can be used instead of a  normal count vector. TF-IDF vectors weight words that appear the most in a given document but negatively weight words that appear in many documents.


\subsection{Naive Bayes}

Using the Naive Bayes model for text classification introduces another method to improve accuracy. While an effective model, our data should be preprocessed to ignore many of the random noise words included in most tweets. With each ineffective term in the training set, the overall effectiveness of Naive Bayes decreases. When training on a toponym heavy dataset like Wikipedia, Naive Bayes can be quite effective.




\chapter{Examples/Results}
**Still in progress**
\section{Examples}
TODO: Find some interesting examples from testing

\section{Results}
\subsection{KNN/Grid Labeling Results}
As shown in the table below, K-Nearest Neighbor increases accuracy with an increase in clusters. With both labeling schemes, they don't appear to improve beyond K values of 128. K values of 128 appear to slightly dominate choosing exclusively the majority label.



\begin{table}
\centering 
  \begin{tabular}{| c | c | c |}
  \hline
  \multicolumn{3}{ |c| }{K-Nearest Neighbor Test Data Results} \\
  \hline      
  Labeling Grid: & 2$^{\circ}$ x2$^{\circ}$ &  10$^{\circ}$x10$^{\circ}$  \\
  Unique Labels: & 623 & 139  \\
  \hline                 
  Majority  & 0.0632124352332  & 0.168604651163 \\
  2 NN & 0.0103626943005 & 0.094476744186 \\
  4 NN & 0.0129533678756 & \textbf{0.180717054264} \\
  8 NN & 0.0150259067358 & 0.101259689922 \\
  16 NN & 0.0290155440415 & 0.140988372093 \\
  32 NN & 0.0461139896373 & \textbf{0.176356589147} \\
  64 NN & 0.059067357513 & 0.132751937984 \\
  128 NN & \textbf{0.0652849740933} & \textbf{0.171511627907} \\
  256 NN & \textbf{0.0678756476684} & \textbf{0.184108527132} \\
  512 NN & \textbf{0.0652849740933} & \textbf{0.179748062016} \\
  \hline  
  \end{tabular}

\end{table}


\subsection{State Boundary Results}
\subsection{K-dimensional Tree Results}
\subsection{Naive Bayes Results}

\subsection{Discussion on Preprocessing}


\chapter{Conclusion}


\appendix
% \chapter{Data Source}
\nocite{*}
\bibliographystyle{plain}
\bibliography{sources}

\end{document}

